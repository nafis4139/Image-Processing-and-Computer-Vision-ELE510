{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B M Nafis Fuad**\n",
    "\n",
    "**ID: 274502**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# ELE510 Image Processing with robot vision: LAB, Exercise 2, Image Formation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "**Purpose:** *To learn about the image formation process, i.e. how images are projected from the scene to the image plane.*\n",
    "\n",
    "The theory for this exercise can be found in chapter 2 and 3 of the text book [1]. Supplementary information can found in chapter 1, 2 and 3 in the compendium [2]. See also the following documentations for help:\n",
    "- [OpenCV](https://docs.opencv.org/4.8.0/d6/d00/tutorial_py_root.html)\n",
    "- [numpy](https://numpy.org/doc/stable/)\n",
    "- [matplotlib](https://matplotlib.org/stable/users/index.html)\n",
    "\n",
    "**IMPORTANT:** Read the text carefully before starting the work. In\n",
    "many cases it is necessary to do some preparations before you start the work\n",
    "on the computer. Read necessary theory and answer the theoretical part\n",
    "frst. The theoretical and experimental part should be solved individually.\n",
    "The notebook must be approved by the lecturer or his assistant.\n",
    "\n",
    "**Approval:**\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "The current notebook should be submitted on CANVAS as a single pdf file. \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    To export the notebook in a pdf format, goes to File -> Download as -> PDF via LaTeX (.pdf).\n",
    "</div>\n",
    "\n",
    "**Note regarding the notebook**: The theoretical questions can be answered directly on the notebook using a *Markdown* cell and LaTex commands (if relevant). In alternative, you can attach a scan (or an image) of the answer directly in the cell.\n",
    "\n",
    "Possible ways to insert an image in the markdown cell:\n",
    "\n",
    "`![image name](\"image_path\")`\n",
    "\n",
    "`<img src=\"image_path\" alt=\"Alt text\" title=\"Title text\" />`\n",
    "\n",
    "\n",
    "**Under you will find parts of the solution that is already programmed.**\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <p>You have to fill out code everywhere it is indicated with `...`</p>\n",
    "    <p>The code section under `######## a)` is answering subproblem a) etc.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\n",
    "## Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "**a)** What is the meaning of the abbreviation PSF? What does the PSF specify?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PSF** - Point Spread Function\n",
    "\n",
    "PSF specifies the shape that a point will take on the image plane. It specifies how an ideal point source of light is transformed or spread out in the final image due to various factors in the imaging system. It characterizes the blur or spreading of light from a point source as it passes through the optical components of the imaging system, including lenses, apertures, and other elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "**b)** Use the imaging model shown in Figure 1. The camera has a lens with focal length $f = 40\\text{mm}$ and in the image plane a CCD sensor of size $10\\text{mm} \\times 10\\text{mm}$. The total number of pixels is $5000 \\times 5000$. At a distance of $z_w = 0.5\\text{m}$ from the camera center, what will be the camera's resolution in pixels per millimeter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<img src=\"perspectiveProjection.jpg\" alt=\"Alt text\" title=\"Title text\" />\n",
    "\n",
    "**Figure 1**: Perspective projection caused by a pinhole camera. Figure 2.23 in [2].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The camera's resolution in pixel per milimeter can be calculated using the following formula,\n",
    "\n",
    "    R0 = R1 / M\n",
    "\n",
    "    Here,\n",
    "    R0 = Camera resolution in object plane\n",
    "    R1 = Camera resolution in image plane\n",
    "    M = Magnification of camera lens\n",
    "\n",
    "**Calculating R1**\n",
    "\n",
    "Resolution of camera at image plane can be calculated by taking the square root of the pixel count and multiplying it by the aspect ratio.\n",
    "\n",
    "    Here,\n",
    "    Pixel count = 5000 x 5000\n",
    "    Sensor Size = 10 mm x 10 mm\n",
    "    So, the aspect ratio is 1:1 \n",
    "\n",
    "    Resolution of camera at image plane = sqrt(5000 x 5000) x 1 = 5000\n",
    "\n",
    "This means that the camera can capture 5000 pixels along each side of the sensor. To find the resolution in pixels per millimeter, we can divide this number by the sensor height size:\n",
    "\n",
    "    R1 = 5000 pixels / 10 mm = 500 pixel/mm\n",
    "\n",
    "*So, the camera‚Äôs resolution at image plane, R1 = 500 pixels per milimeter*\n",
    "\n",
    "\n",
    "**Calculating M**\n",
    "\n",
    "Magnification of lens can be calculated using the following formula,\n",
    "\n",
    "    M = zw / f\n",
    "\n",
    "    Here,\n",
    "    zw = Distance from camera center to the object = 0.5 m = 500 mm\n",
    "    f = Focal length = 40 mm\n",
    "\n",
    "    M = 500 mm / 40 mm = 12.5\n",
    "\n",
    "This means that the object size is magnified by 12.5 times on the image plane. Therefore, the resolution at the object plane is 12.5 times smaller than the resolution at the image plane.\n",
    "\n",
    "\n",
    "**Calculating R0**\n",
    "\n",
    "    R0 = R1 / M = 500 pixel per milimter / 12.5 = 40 pixel per milimeter\n",
    "\n",
    "The camera‚Äôs resolution in pixels per millimeter at a distance of ùëßùë§=0.5m from the camera center is 40.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "**c)** Explain how a Bayer filter works. What is the alternative to using this type of filter in image acquisition?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Bayer Filter works by filtering light to capture color information in a way that mimics the human eye's sensitivity to different colors. Here's how it works:\n",
    "\n",
    "* A Bayer filter is a grid of tiny color filters placed over the pixels of an image sensor. These filters are typically arranged in a checkerboard pattern, with alternating red, green, and blue filters. Each pixel on the sensor is covered by one of these filters.\n",
    "* The human eye perceives color using three types of color receptors: red, green, and blue cones. Similarly, the Bayer filter uses red, green, and blue filters to mimic this color perception. The green filter is used twice as much as red and blue because our eyes are more sensitive to green light.\n",
    "* When light enters the camera's sensor through the lens, each pixel captures the intensity of light for one of the three colors: red, green, or blue, depending on which filter covers it. For example, pixels with red filters record the intensity of red light.\n",
    "* Since each pixel on the sensor only captures one color's information, the missing color information for each pixel is interpolated (estimated) using data from neighboring pixels. This process is known as demosaicing, and various algorithms are used to interpolate the missing color values.\n",
    "\n",
    "Alternative of Bayes Filter can be 3CCD where cameras often use three separate image sensors, each with a dedicated filter for one of the primary colors (red, green, and blue). This eliminates the need for interpolation and can provide higher color accuracy but is more complex and expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)** Briefly explain the following concepts: Sampling, Quantization, Gamma Compression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sampling:** Sampling is the process of converting a continuous image into a grid of discrete points or pixels. It determines the spatial resolution of the digital image by specifying how densely the image is sampled. Higher sampling rates result in more detail but also larger file sizes.\n",
    "\n",
    "**Quantization:** Quantization involves mapping the continuous range of brightness values in an image to a limited set of discrete values. It reduces the image's data size by approximating continuous intensity levels with a fixed number of digital values (typically integers).\n",
    "\n",
    "**Gamma Compression:** Gamma compression (or gamma correction) is a nonlinear adjustment applied to pixel values in an image. It compensates for the nonlinear response of human vision to changes in brightness. Gamma correction ensures that the displayed image appears more natural by adjusting its tonal response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Problem 2\n",
    "\n",
    "Assume we have captured an image with a digital camera. The image covers an area in the scene of size $1.024\\text{m} \\times 0.768\\text{m}$ (The camera has been pointed towards a wall such that the distance is approximately constant over the whole image plane, *weak perspective*). The camera has 4096 pixels horizontally, and 3072 pixels vertically. The active region on the CCD-chip is $8\\text{mm} \\times 6\\text{mm}$. We define the spatial coordinates $(x_w,y_w)$ such that the origin is at the center of the optical axis, x-axis horizontally and y-axis vertically upwards. The image indexes $(x,y)$ is starting in the upper left corner. The solutions to this problem can be found from simple geometric considerations. Make a sketch of the situation and answer the following questions:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** What is the size of each sensor (one pixel) on the CCD-chip?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sensor Size (x, y) = 8 mm x 6 mm\n",
    "* Pixel Number (M, N) = 4096 x 3072\n",
    "\n",
    "The size of each sensor (one pixel) on the CCD-chip can be calculated as follows:\n",
    "\n",
    "      Sensor Size (in the horizontal direction), dx = (Sensor Width, x) / (Number of Pixels Horizontally, M)\n",
    "\n",
    "            = (8mm) / (4096 pixels) = 0.001953125 mm/pixel\n",
    "\n",
    "      Sensor Size (in the vertical direction), dy = (Sensor Height, y) / (Number of Pixels Vertically, N)\n",
    "    \n",
    "            = (6mm) / (3072 pixels) = 0.001953125 mm/pixel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** What is the scaling coefficient between the image plane (CCD-chip) and the scene? What is the scaling coefficient between the scene coordinates and the pixels of the image?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Field of View, FOV (X, Y) = 1.024 m √ó 0.768 m\n",
    "* Sensor Size (x, y) = 8 mm x 6 mm\n",
    "* Pixel Number (M, N) = 4096 x 3072\n",
    "\n",
    "Scaling coefficient between the image plane (CCD-chip) and the scene can be calculated as follows:\n",
    "\n",
    "    Horizontal Scaling Coefficient, sx = (Sensor Width, x) / (Scene Width, X) \n",
    "\n",
    "        = 8mm / 1.024 m = 8mm / 1024mm = 0.0078125\n",
    "\n",
    "    Vertical Scaling Coefficient, sy = (Sensor Height, y) / (Scene Height, Y) \n",
    "\n",
    "        = 6mm / 0.768m = 6mm / 768mm = 0.0078125\n",
    "\n",
    "Scaling coefficient between the scene coordinates and the pixels of the image can be calculated as follows:\n",
    "\n",
    "    Horizontal Scaling Coefficient, alpha_x = sx / dx \n",
    "\n",
    "        = (0.0078125) / (0.001953125 mm/pixel) = 4 pixel per milimeter\n",
    "\n",
    "    Vertical Scaling Coefficient, alpha_y = sy / dy\n",
    "        \n",
    "        = (0.0078125) / (0.001953125 mm/pixel) = 4 pixel per milimeter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Problem 3\n",
    "\n",
    "Translation from the scene to a camera sensor can be done using a transformation matrix, $T$. \n",
    "\n",
    "\\begin{equation}\n",
    "    \\left[\n",
    "\t\\begin{array}{c}x \\\\ y \\\\ 1\\end{array}\\right] = \n",
    "\tT\\left[\n",
    "\t\\begin{array}{ccc}\n",
    "\t\tx_w\\\\ y_w\\\\ 1\n",
    "\t\\end{array} \\right]\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation}\n",
    "\tT= \\left[\\begin{array}{ccc} \\alpha_x & 0 & x_0\\\\\n",
    "\t\t\t0 & \\alpha_y & y_0\\\\\n",
    "\t\t0   & 0 & 1\n",
    "\t\\end{array} \\right]\n",
    "\\end{equation}\n",
    "$\\alpha_x$ and $\\alpha_y$ are the scaling factors for their corresponding axes.\n",
    "\n",
    "Write a function in Python that computes the image points using the transformation matrix, using the parameters from Problem 2. Let the input to the function be a set of $K$ scene points, given by a $2 \\times K$ matrix, and the output the resulting image points also given by a $2 \\times K$ matrix. The parameters defining the image sensor and field of view from the camera center to the wall can also be given as input parameters.  For simplicity, let the optical axis $(x_0,y_0)$ meet the image plane at the middle point (in pixels).\n",
    "\n",
    "Test the function for the following input points given as a matrix:\n",
    "\\begin{equation}\n",
    "    {\\mathbf P}_{in} = \\left[\\begin{array}{ccccccccc} \n",
    "    0.512 & -0.512 & -0.512 & 0.512 & 0 & 0.35 & 0.35 & 0.3 & 0.7\\\\\n",
    "    0.384 & 0.384 & -0.384 & -0.384 & 0 & 0.15 & -0.15 & -0.5 & 0\\end{array}\\right]\n",
    "\\end{equation}\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "Comment on the results, especially notice the two last points!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the packages that are useful inside the definition of the weakPerspective function\n",
    "import math \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that takes in input:\n",
    "# - FOV: field of view,\n",
    "# - sensorsize: size of the sensor,\n",
    "# - n_pixels: camera pixels,\n",
    "# - p_scene: K input points (2xK matrix)\n",
    "# and return the resulting image points given the 2xK matrix\n",
    "\n",
    "def weakPerspective(FOV, sensorsize, n_pixels, p_scene):\n",
    "    #Calculate size of each sensor (one pixel)\n",
    "    dx = sensorsize[0] / n_pixels[0]\n",
    "    dy = sensorsize[1] / n_pixels[1]\n",
    "\n",
    "    #Calculate the scaling coefficient between the image plane (CCD-chip) and the scene\n",
    "    sx = sensorsize[0] / FOV[0]\n",
    "    sy = sensorsize[1] / FOV[1]\n",
    "    \n",
    "    # Calculate the scaling factors\n",
    "    alpha_x = sx / dx\n",
    "    alpha_y = sy / dy\n",
    "\n",
    "    # Calculate the center pixel\n",
    "    x0 = n_pixels[0] / 2\n",
    "    y0 = n_pixels[1] / 2\n",
    "\n",
    "    # Construct the transformation matrix T\n",
    "    T = np.array([[alpha_x, 0, x0],\n",
    "                  [0, alpha_y, y0],\n",
    "                  [0, 0, 1]])\n",
    "    \n",
    "    # Add a row of ones to the input points matrix\n",
    "    p_scene = np.vstack((p_scene, np.ones(p_scene.shape[1])))\n",
    "\n",
    "    # Apply the transformation matrix to the input points\n",
    "    p_image = T @ p_scene\n",
    "    \n",
    "    # Divide by the third row to get homogeneous coordinates\n",
    "    p_image = p_image / p_image[2,:]\n",
    "    \n",
    "    # Remove the third row and return the result\n",
    "    p_image = p_image[0:2,:]\n",
    "    \n",
    "    return p_image    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above function is then called using the following parameters:\n",
    "\n",
    "# Parameters\n",
    "FOV = (1.024, 0.768)  # Field of view in meters (width x height)\n",
    "sensorsize = (8 / 1000, 6 / 1000)  # Sensor size in meters (width x height)\n",
    "n_pixels = (4096, 3072)  # Number of pixels (width x height)\n",
    "p_scene_x = [0.512, -0.512, -0.512, 0.512, 0, 0.35, 0.35, 0.3, 0.7]\n",
    "p_scene_y = [0.384, 0.384, -0.384, -0.384, 0, 0.15, -0.15, -0.5, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4096.    0.    0. 4096. 2048. 3448. 3448. 3248. 4848.]\n",
      " [3072. 3072.    0.    0. 1536. 2136.  936. -464. 1536.]]\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "# This cell is locked; it can be only be executed to see the results. \n",
    "####\n",
    "# Input data:\n",
    "p_scene = np.array([p_scene_x, p_scene_y])\n",
    "\n",
    "# Call to the weakPerspective() function \n",
    "pimage = weakPerspective(FOV, sensorsize, n_pixels, p_scene)\n",
    "\n",
    "# Result: \n",
    "print(pimage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments**\n",
    "\n",
    "* The first four points indicates the four corners (0 x 0), (0 x 3072), (4096 x 3072), (4096 x 0)\n",
    "* The fifth point indicates the center pixel (2048 x 1536)\n",
    "* The sixth and seventh points are within the FOV, so they will be captured in the image sensor (3448 x 2136) & (3448 x 936)\n",
    "* The last two points are outside the FOV, they won't be captured in the image sensor (3248 x -464) & (4848 x 1536)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\n",
    "\n",
    "### Delivery (dead line) on CANVAS: 12-09-2021 at 23:59\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Contact\n",
    "### Course teacher\n",
    "Professor Kjersti Engan, room E-431,\n",
    "E-mail: kjersti.engan@uis.no\n",
    "\n",
    "### Teaching assistant\n",
    "Saul Fuster Navarro, room E-401\n",
    "E-mail: saul.fusternavarro@uis.no\n",
    "\n",
    "\n",
    "Jorge Garcia Torres Fernandez, room E-401\n",
    "E-mail: jorge.garcia-torres@uis.no"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\n",
    "## References\n",
    "\n",
    "[1] S. Birchfeld, Image Processing and Analysis. Cengage Learning, 2016.\n",
    "\n",
    "[2] I. Austvoll, \"Machine/robot vision part I,\" University of Stavanger, 2018. Compendium, CANVAS."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
